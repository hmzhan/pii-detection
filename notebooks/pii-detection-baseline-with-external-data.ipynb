{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":7443360,"sourceType":"datasetVersion","datasetId":4332496},{"sourceId":159367535,"sourceType":"kernelVersion"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### This notebook is modified from <a href=\"https://www.kaggle.com/code/leonshangguan/modify-of-pii-detect-study\">Modify of PII Detect Study</a>, <a href=\"https://www.kaggle.com/code/pjmathematician/pii-eda-presidio-baseline\">PII EDA Presidio Baseline</a> and <a href=\"https://www.kaggle.com/code/yunsuxiaozi/pii-detect-study-notebook\">PII detect study notebook</a>. ","metadata":{}},{"cell_type":"markdown","source":"# Modifications \n\nFirstly, big thanks to the users who provided the notebooks above. This notebook is merely adding some utility code to make a solid baseline that other users may iterate on, but all the heavy lifting was done by the above notebooks.\n\nI encapsulated the analyzer in a class, and added code to run the analyzer on (potentially) both the training and test set. I also added validation code, so that we can analyze the performance of the analyzer on the training set.\n\nI also added a global configuration for ease of testing, which allows the user to switch between training and inference mode. Additionally, I incorporated the external data that [https://www.kaggle.com/alejopaullier](@moth) kindly provided in his discussion post.\n\nFor now, the business logic roughly stays the same as the Modify of PII Detect Study notebook that I used beforehand.\n\n## Resources\n\n* Customizing the presidio analyzer: https://microsoft.github.io/presidio/samples/python/customizing_presidio_analyzer/","metadata":{}},{"cell_type":"markdown","source":"# Version History\n\n- v16: Original baseline\n- v17: Changed score thresholds for patterns from 0.5 -> 0.8\n- v20: fixed bug in evaluation code","metadata":{}},{"cell_type":"markdown","source":"# Configuration","metadata":{}},{"cell_type":"code","source":"class CONFIG:\n    \"\"\"\n    > General Options\n    \"\"\"\n    # global seed\n    seed = 42\n    # number of samples to use for testing purposes\n    # if None, we use the whole training dataset\n    samples_testing = None\n    # flag to indicate whether to use the external training dataset\n    # or just to use the original data\n    use_external_train_data = True\n    # whether to run the algorithm on the training set and do subsequent validation\n    # with 6.8k rows, this takes almost 50 minutes to run\n    run_on_train_data = True\n    \n    \"\"\"\n    > Analyzer Options\n    \"\"\"\n    # score threshold for patterns\n    address_pattern_score = 0.8\n    email_pattern_score = 0.8\n    url_pattern_score = 0.8","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:30:28.90152Z","iopub.execute_input":"2024-01-21T15:30:28.902634Z","iopub.status.idle":"2024-01-21T15:30:28.93608Z","shell.execute_reply.started":"2024-01-21T15:30:28.902594Z","shell.execute_reply":"2024-01-21T15:30:28.934761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"markdown","source":"### Install presidio","metadata":{}},{"cell_type":"code","source":"#安装python库 presidio_analyzer 不从python库里下载,而是从给定的链接处下载,更新到最新版本,并减少输出信息.\n!pip install -U -q presidio_analyzer --no-index --find-links=file:///kaggle/input/presidio-wheels/presidio","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-21T15:30:28.93757Z","iopub.execute_input":"2024-01-21T15:30:28.938132Z","iopub.status.idle":"2024-01-21T15:30:48.095605Z","shell.execute_reply.started":"2024-01-21T15:30:28.938086Z","shell.execute_reply":"2024-01-21T15:30:48.094277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Import  necessary libraries","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\nfrom presidio_analyzer import AnalyzerEngine\nfrom presidio_analyzer.nlp_engine import NlpEngineProvider\nfrom tqdm import tqdm\nfrom typing import List\nimport random\nimport pprint\nimport re\nimport gc\nfrom ast import literal_eval\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.metrics import fbeta_score, classification_report, confusion_matrix\n\nfrom presidio_analyzer import AnalyzerEngine, PatternRecognizer, EntityRecognizer, Pattern, RecognizerResult\nfrom presidio_analyzer.recognizer_registry import RecognizerRegistry\nfrom presidio_analyzer.nlp_engine import NlpEngine, SpacyNlpEngine, NlpArtifacts\nfrom presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n\nfrom dateutil import parser","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:30:48.097454Z","iopub.execute_input":"2024-01-21T15:30:48.098398Z","iopub.status.idle":"2024-01-21T15:30:54.068836Z","shell.execute_reply.started":"2024-01-21T15:30:48.098353Z","shell.execute_reply":"2024-01-21T15:30:54.067607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:30:54.072705Z","iopub.execute_input":"2024-01-21T15:30:54.073448Z","iopub.status.idle":"2024-01-21T15:30:54.078593Z","shell.execute_reply.started":"2024-01-21T15:30:54.073403Z","shell.execute_reply":"2024-01-21T15:30:54.077335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Metric (F1 Beta Score)","metadata":{}},{"cell_type":"code","source":"def score(y_true, y_pred):\n    return fbeta_score(y_true, y_pred, average=\"micro\", beta=5)\n\ndef macro_score(y_true, y_pred):\n    return fbeta_score(y_true, y_pred, average=\"macro\", beta=5)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:34:29.005167Z","iopub.execute_input":"2024-01-21T15:34:29.005564Z","iopub.status.idle":"2024-01-21T15:34:29.011344Z","shell.execute_reply.started":"2024-01-21T15:34:29.005526Z","shell.execute_reply":"2024-01-21T15:34:29.010388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import Datasets","metadata":{}},{"cell_type":"markdown","source":"## Import Original Data","metadata":{}},{"cell_type":"code","source":"train_df = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\nprint(f\"len(train_df):{len(train_df)}, train_df[0].keys(): {list(train_df[0].keys())}\")\nprint(\"-\"*50)\n\nlabels = set()\nfor i in range(len(train_df)):\n    labels.update(train_df[i]['labels'])\nprint(f\"labels: {labels}\")\n\ntest_df = json.load(open('/kaggle/input/pii-detection-removal-from-educational-data/test.json'))","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:30:54.092269Z","iopub.execute_input":"2024-01-21T15:30:54.092918Z","iopub.status.idle":"2024-01-21T15:30:56.692301Z","shell.execute_reply.started":"2024-01-21T15:30:54.092887Z","shell.execute_reply":"2024-01-21T15:30:56.691106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load External Data (if needed)","metadata":{}},{"cell_type":"code","source":"if CONFIG.use_external_train_data:\n    # Convert the \"stringified lists\" in the columns to proper Python lists\n    df_train_external = pd.read_csv('/kaggle/input/pii-external-dataset/pii_dataset.csv', converters={\n        'tokens': literal_eval, \n        'labels': literal_eval, \n        'trailing_whitespace': literal_eval\n    })\n    df_train_external.rename(columns={'text': 'full_text'}, inplace=True)\n    # convert to format similar to how we load in the original data\n    df_train_external = df_train_external.to_dict('records')\n    train_df.extend(df_train_external)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:30:56.693701Z","iopub.execute_input":"2024-01-21T15:30:56.694101Z","iopub.status.idle":"2024-01-21T15:31:03.139165Z","shell.execute_reply.started":"2024-01-21T15:30:56.694069Z","shell.execute_reply":"2024-01-21T15:31:03.138232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sample Data (if needed)","metadata":{}},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.140699Z","iopub.execute_input":"2024-01-21T15:31:03.141141Z","iopub.status.idle":"2024-01-21T15:31:03.149592Z","shell.execute_reply.started":"2024-01-21T15:31:03.141101Z","shell.execute_reply":"2024-01-21T15:31:03.148632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.samples_testing != None:\n    train_df = random.sample(train_df, CONFIG.samples_testing)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.150949Z","iopub.execute_input":"2024-01-21T15:31:03.151285Z","iopub.status.idle":"2024-01-21T15:31:03.260426Z","shell.execute_reply.started":"2024-01-21T15:31:03.151259Z","shell.execute_reply":"2024-01-21T15:31:03.259134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.263917Z","iopub.execute_input":"2024-01-21T15:31:03.264313Z","iopub.status.idle":"2024-01-21T15:31:03.273987Z","shell.execute_reply.started":"2024-01-21T15:31:03.26428Z","shell.execute_reply":"2024-01-21T15:31:03.273062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Methods","metadata":{}},{"cell_type":"code","source":"def is_valid_date(text):\n    try:\n        # Attempt to parse the text as a date\n        parsed_date = parser.parse(text)\n        return True\n    except:\n        return False\n    \ndef tokens2index(row):\n    tokens  = row['tokens']\n    start_ind = []\n    end_ind = []\n    prev_ind = 0\n    for tok in tokens:\n        start = prev_ind + row['full_text'][prev_ind:].index(tok)\n        end = start+len(tok)\n        start_ind.append(start)\n        end_ind.append(end)\n        prev_ind = end\n    return start_ind, end_ind\n\n# binary search\ndef find_or_next_larger(arr, target):\n    left, right = 0, len(arr) - 1\n\n    while left <= right:\n        mid = (left + right) // 2\n\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return left\n\ndef count_trailing_whitespaces(word):\n    return len(word) - len(word.rstrip())","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.275763Z","iopub.execute_input":"2024-01-21T15:31:03.276283Z","iopub.status.idle":"2024-01-21T15:31:03.286532Z","shell.execute_reply.started":"2024-01-21T15:31:03.276245Z","shell.execute_reply":"2024-01-21T15:31:03.285733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create Analyzer\n\nFor ease of code, we encapsulate the analyzer code in a class.","metadata":{}},{"cell_type":"code","source":"class MyAnalyzer:\n    \n    def __init__(self):\n        ## Initialize the analyzer\n        configuration = {\n            \"nlp_engine_name\": \"spacy\",\n            \"models\": [{\"lang_code\": \"en\", \"model_name\": \"en_core_web_lg\"}],\n        }\n        \n        # Create NLP engine based on configuration\n        provider = NlpEngineProvider(nlp_configuration=configuration)\n        nlp_engine = provider.create_engine()\n\n        # create address recognizer\n        address_regex = r'\\b\\d+\\s+\\w+(\\s+\\w+)*\\s+((st(\\.)?)|(ave(\\.)?)|(rd(\\.)?)|(blvd(\\.)?)|(ln(\\.)?)|(ct(\\.)?)|(dr(\\.)?))\\b'\n        address_pattern = Pattern(name=\"address\", regex=address_regex, score = CONFIG.address_pattern_score)\n        address_recognizer = PatternRecognizer(supported_entity=\"ADDRESS_CUSTOM\", patterns = [address_pattern])\n\n        # create email recognizer\n        email_regex = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n        email_pattern = Pattern(name=\"email address\", regex=email_regex, score = CONFIG.email_pattern_score)\n        email_recognizer = PatternRecognizer(supported_entity=\"EMAIL_CUSTOM\", patterns = [email_pattern])\n\n        # create url recognizer \n        url_regex = r'https?://\\S+|www\\.\\S+'\n        url_pattern = Pattern(name=\"url\", regex=url_regex, score=CONFIG.url_pattern_score)\n        url_recognizer = PatternRecognizer(supported_entity=\"URL_CUSTOM\", patterns = [url_pattern])\n\n        registry = RecognizerRegistry()\n        registry.load_predefined_recognizers()\n        registry.add_recognizer(address_recognizer)\n        registry.add_recognizer(email_recognizer)\n        registry.add_recognizer(url_recognizer)\n\n        # Pass the created NLP engine and supported_languages to the AnalyzerEngine\n        self.analyzer = AnalyzerEngine(\n            nlp_engine=nlp_engine, \n            supported_languages=[\"en\"],\n            registry=registry\n        )\n        \n        ## Initialize the black list\n        self.black_list = [\"wikipedia\", \"coursera\", \".pdf\", \".PDF\", \"article\", \n                           \".png\", \".gov\", \".work\", \".ai\", \".firm\", \".arts\", \n                           \".store\", \".rec\", \".biz\", \".travel\" ]\n        \n     \n    def predict_tokens(self, df_: list) -> pd.DataFrame:\n        \"\"\"Predict the tokens that have PII in the dataframe.\"\"\"\n        \n        PHONE_NUM, NAME_STUDENT, URL_PERSONAL, EMAIL, STREET_ADDRESS, ID_NUM, USERNAME = [],[],[],[],[],[], []\n\n        preds = []\n        #查找每个词分词后的起始位置和终点位置\n        for i in tqdm(range(len(df_)), desc=\"Processing tokens2index\"):\n            start, end = tokens2index(df_[i])\n            df_[i]['start'] = start\n            df_[i]['end'] = end\n\n        for i, d in tqdm(enumerate(df_), total=len(df_), desc=\"Analyzing entities\"):\n            #results:[type: PERSON, start: 22, end: 37, score: 0.85]\n            results = self.analyzer.analyze(text=d['full_text'],\n                                   entities=[\n                                             #\"PHONE_NUMBER\", \n                                             \"PERSON\", \n                                             \"URL_CUSTOM\", #\"IP_ADDRESS\", #\"URL\",\n                                             \"EMAIL_ADDRESS\", \"EMAIL_CUSTOM\", \n                                             \"ADDRESS_CUSTOM\",\n                                             \"US_SSN\", \"US_ITIN\", \"US_PASSPORT\", \"US_BANK_NUMBER\",\n                                             \"USERNAME\"],\n                                   language='en',\n        #                            score_threshold=0.2,\n                                    )\n            pre_preds = []\n            for r in results:#遍历找到过的每个实体,r:[type: PERSON, start: 22, end: 37, score: 0.85]\n                #就是第s个词就是某个实体的开始\n                s = find_or_next_larger(d['start'], r.start)#d['start'][s]=r.start\n                end = r.end#实体终点\n                word = d['full_text'][r.start:r.end]#文本里找单词\n                end = end - count_trailing_whitespaces(word)#end减去尾部的空格就是单词自身尾部的下标\n                temp_preds = [s]#实体单词的集合从第s个单词开始,然后连续几个单词?\n                try:\n                    #实体可能不是一个单词,分词的下一个单词如果还没有到达实体的尾部,就把下一个单词加上\n                    while d['end'][s+1] <= end:\n                        temp_preds.append(s+1)\n                        s +=1\n                except:\n                    pass\n\n                #找出来的实体是什么,我们就给它打对应的标签\n                tmp = False\n\n                if r.entity_type == 'USERNAME':\n                    label =  'USERNAME'\n                    USERNAME.append(d['full_text'][r.start:r.end])\n\n        #         if r.entity_type == 'PHONE_NUMBER':\n        #             #检查是不是日期类型\n        #             if is_valid_date(word):\n        #                 continue\n        #             label =  'PHONE_NUM'\n        #             PHONE_NUM.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'PERSON':\n                    label =  'NAME_STUDENT'\n                    NAME_STUDENT.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'ADDRESS_CUSTOM':\n                    label = 'STREET_ADDRESS'\n                    STREET_ADDRESS.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'US_SSN' or r.entity_type == 'US_ITIN' or r.entity_type == 'US_PASSPORT' or r.entity_type == 'US_BANK_NUMBER':\n                    label = 'ID_NUM'\n                    ID_NUM.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'EMAIL_ADDRESS' or r.entity_type == 'EMAIL_CUSTOM':\n                    label = 'EMAIL'\n                    EMAIL.append(d['full_text'][r.start:r.end])\n\n                if r.entity_type == 'URL_CUSTOM':# or r.entity_type == 'IP_ADDRESS' or \"http\" in word:\n                    #去除掉黑名单里的标签\n                    for w in self.black_list:\n                        if w in word:\n                            tmp = True\n                            break\n\n                    label = 'URL_PERSONAL'\n                    URL_PERSONAL.append(d['full_text'][r.start:r.end])\n\n                if tmp:\n                    continue\n\n\n                #取出实体中的一个分词的下标\n                for p in temp_preds:\n                    if len(pre_preds) > 0:#第2次及以后经过这里.\n                        \"\"\"\n                        新开始一个r的时候,pre_preds[-1]['rlabel']还是上一个实体的r.entity_type\n                        此时也许会不等于这个实体的r.entity_type,换句话说,第一个等号就是还在同一个实体里.\n                        p - pre_preds[-1]['token']==1就是连续的意思\n                        \"\"\"\n                        if pre_preds[-1]['rlabel'] == r.entity_type and (p - pre_preds[-1]['token']==1):\n                            label_f = \"I-\"+label#实体的中间位置\n                        else:\n                            label_f = \"B-\"+label#否则就是下一个实体的开始\n                    else:#第一个label是起始位置,故标记为‘B-’\n                        label_f = \"B-\"+label\n                    #保存document,从第p个单词开始,标签为label_f\n                    pre_preds.append(({\n                            \"document\":d['document'],\n                            \"token\":p,\n                            \"label\":label_f,\n                            \"rlabel\":r.entity_type#实体的类型\n                        }))\n            preds.extend(pre_preds)#遍历完这个数据之后,将所有找到的实体做汇总\n            \n        preds_df = pd.DataFrame(preds).iloc[:,:-1].reset_index()\n        return preds_df\n        \n    \n        ","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.288534Z","iopub.execute_input":"2024-01-21T15:31:03.289339Z","iopub.status.idle":"2024-01-21T15:31:03.314709Z","shell.execute_reply.started":"2024-01-21T15:31:03.289299Z","shell.execute_reply":"2024-01-21T15:31:03.313597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Train Set","metadata":{}},{"cell_type":"code","source":"analyzer = MyAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:03.315841Z","iopub.execute_input":"2024-01-21T15:31:03.316173Z","iopub.status.idle":"2024-01-21T15:31:07.797803Z","shell.execute_reply.started":"2024-01-21T15:31:03.316128Z","shell.execute_reply":"2024-01-21T15:31:07.796913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    train_preds = analyzer.predict_tokens(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:31:07.799098Z","iopub.execute_input":"2024-01-21T15:31:07.799505Z","iopub.status.idle":"2024-01-21T15:33:24.553893Z","shell.execute_reply.started":"2024-01-21T15:31:07.79947Z","shell.execute_reply":"2024-01-21T15:33:24.552712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate Performance on Training Set","metadata":{}},{"cell_type":"markdown","source":"## Generate Corresponding DataFrame for \"True\" Answers","metadata":{}},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    \n    train_act_records = []\n    count = 0\n    for entry in train_df:\n        for idx, (token, label) in enumerate(zip(entry[\"tokens\"], entry[\"labels\"])):\n            if label != 'O':\n                train_act_records.append({\n                    'row_id': count,\n                    'document': entry[\"document\"],\n                    'token': idx,\n                    'label': label,\n                })\n                count += 1\n\n    train_act = pd.DataFrame.from_records(train_act_records)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:33:24.55513Z","iopub.execute_input":"2024-01-21T15:33:24.555447Z","iopub.status.idle":"2024-01-21T15:33:24.697279Z","shell.execute_reply.started":"2024-01-21T15:33:24.55542Z","shell.execute_reply":"2024-01-21T15:33:24.696212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_pred_act_lists_for_dfs(preds: pd.DataFrame, act: pd.DataFrame):\n    document_idx_list = [(ex[\"document\"], len(ex[\"tokens\"])) for ex in train_df]\n    \n    preds_list = []\n    act_list = []\n    preds.sort_values(by=\"token\", inplace=True)\n    act.sort_values(by=\"token\", inplace=True)\n    \n    for document, len_tokens in tqdm(document_idx_list, total=len(document_idx_list)):\n        preds_doc = preds[preds[\"document\"] == document]\n        act_doc = act[act[\"document\"] == document]\n        \n        # We do a \"merge\" (like in mergesort) to combine the results from the preds and \n        # actual values\n        preds_idx = 0\n        act_idx = 0\n        for i in range(len_tokens):\n            preds_head, act_head = None, None\n            if preds_idx < len(preds_doc):\n                preds_head = preds_doc.iloc[preds_idx]\n            if act_idx < len(act_doc):\n                act_head = act_doc.iloc[act_idx]\n                \n            if act_head is not None and act_head[\"token\"] == i:\n                act_list.append(act_head[\"label\"])\n                act_idx += 1\n            else:\n                act_list.append('O')\n                \n            if preds_head is not None and preds_head[\"token\"] == i:\n                preds_list.append(preds_head[\"label\"])\n                preds_idx += 1\n            else:\n                preds_list.append('O')\n            \n    return preds_list, act_list\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:33:24.698661Z","iopub.execute_input":"2024-01-21T15:33:24.700472Z","iopub.status.idle":"2024-01-21T15:33:24.710567Z","shell.execute_reply.started":"2024-01-21T15:33:24.70043Z","shell.execute_reply":"2024-01-21T15:33:24.709578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    train_preds_list, train_act_list = get_pred_act_lists_for_dfs(train_preds, train_act)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:33:24.712067Z","iopub.execute_input":"2024-01-21T15:33:24.712418Z","iopub.status.idle":"2024-01-21T15:33:47.98166Z","shell.execute_reply.started":"2024-01-21T15:33:24.712382Z","shell.execute_reply":"2024-01-21T15:33:47.98059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Classification Report on Training Set","metadata":{}},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    print(classification_report(train_preds_list, train_act_list, digits=4))","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:33:47.983494Z","iopub.execute_input":"2024-01-21T15:33:47.983806Z","iopub.status.idle":"2024-01-21T15:33:59.685068Z","shell.execute_reply.started":"2024-01-21T15:33:47.98378Z","shell.execute_reply":"2024-01-21T15:33:59.684069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## F Beta Score on Training Set","metadata":{}},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    print(\"Micro F1 Beta Score:\", score(train_preds_list, train_act_list))\n    print(\"Macro F1 Beta Score:\", macro_score(train_preds_list, train_act_list))","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:35:01.247051Z","iopub.execute_input":"2024-01-21T15:35:01.247479Z","iopub.status.idle":"2024-01-21T15:35:09.116132Z","shell.execute_reply.started":"2024-01-21T15:35:01.247448Z","shell.execute_reply":"2024-01-21T15:35:09.114741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if CONFIG.run_on_train_data:\n    del train_preds_list, train_act_list, train_preds, train_act\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:34:03.607393Z","iopub.execute_input":"2024-01-21T15:34:03.607737Z","iopub.status.idle":"2024-01-21T15:34:03.611496Z","shell.execute_reply.started":"2024-01-21T15:34:03.607709Z","shell.execute_reply":"2024-01-21T15:34:03.610592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predict Test Set","metadata":{}},{"cell_type":"code","source":"test_preds = analyzer.predict_tokens(test_df)\ntest_preds.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:34:03.612808Z","iopub.execute_input":"2024-01-21T15:34:03.613097Z","iopub.status.idle":"2024-01-21T15:34:04.944614Z","shell.execute_reply.started":"2024-01-21T15:34:03.613073Z","shell.execute_reply":"2024-01-21T15:34:04.943533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(test_preds)\nsubmission.columns = ['row_id','document', 'token', 'label']\nsubmission.to_csv('submission.csv', index = False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:34:04.945797Z","iopub.execute_input":"2024-01-21T15:34:04.946113Z","iopub.status.idle":"2024-01-21T15:34:04.960778Z","shell.execute_reply.started":"2024-01-21T15:34:04.946085Z","shell.execute_reply":"2024-01-21T15:34:04.959754Z"},"trusted":true},"execution_count":null,"outputs":[]}]}