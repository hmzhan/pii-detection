{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":160547870,"sourceType":"kernelVersion"}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 🛑 Wait a second - after this you should also look at the inference notebook\n- My inference notebook (containing equally many emojis) is here: I would love an upvote if you use the notebook or learned something new!\n- https://www.kaggle.com/code/valentinwerner/893-deberta3base-inference","metadata":{}},{"cell_type":"markdown","source":"## 🏟️ Credits (because this baseline did mostly already exist when I joiend)\n\n- @Nicholas Broad published the transformer baseline which performs only marginally worse: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854\n- @Joseph Josia published the training notebook which I basically copy pasted (which is based itself on nbroad, but yeah): https://www.kaggle.com/code/takanashihumbert/piidd-deberta-model-starter-training\n\n","metadata":{}},{"cell_type":"markdown","source":"## 💡 What I added\n- Downsampling negative samples (samples without labels, but they possible still work as examples where names should not be tagged as name)\n- Adding @moths external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/469493\n- Adding PJMathematicianss external data: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470921\n- However, I used my cleaned version instead (the punctuation is flawed in the original data set at the time of this trainign): https://www.kaggle.com/code/valentinwerner/fix-punctuation-tokenization-external-dataset\n\nDoing this brought the LB score to .888 - Trained in Kaggle Notebook, no tricks or secrets.\n\n- I added emojis because that seems to be the kaggle upvote meta","metadata":{}},{"cell_type":"markdown","source":"## 📝 Config & Imports\n- 1024 max length has been working well for me. As some samples are longer, you may want to go as high as you can ","metadata":{}},{"cell_type":"code","source":"TRAINING_MODEL_PATH = \"microsoft/deberta-v3-base\"\nTRAINING_MAX_LENGTH = 1024\nOUTPUT_DIR = \"output\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-27T22:11:21.630682Z","iopub.execute_input":"2024-01-27T22:11:21.631202Z","iopub.status.idle":"2024-01-27T22:11:21.642774Z","shell.execute_reply.started":"2024-01-27T22:11:21.631164Z","shell.execute_reply":"2024-01-27T22:11:21.641709Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-01-27T22:11:21.644160Z","iopub.execute_input":"2024-01-27T22:11:21.644631Z","iopub.status.idle":"2024-01-27T22:11:40.903378Z","shell.execute_reply.started":"2024-01-27T22:11:21.644581Z","shell.execute_reply":"2024-01-27T22:11:40.902022Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import json\nimport argparse\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import AutoModelForTokenClassification, DataCollatorForTokenClassification\nimport evaluate\nfrom datasets import Dataset, features\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-01-27T22:11:40.904939Z","iopub.execute_input":"2024-01-27T22:11:40.905271Z","iopub.status.idle":"2024-01-27T22:12:00.138541Z","shell.execute_reply.started":"2024-01-27T22:11:40.905240Z","shell.execute_reply":"2024-01-27T22:12:00.137466Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 🗺️ Data Selection and Label Mapping\n- As mentioned before, I additionaly use the moth dataset","metadata":{}},{"cell_type":"code","source":"data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"))\n\n# downsampling of negative examples\np=[] # positive samples (contain relevant labels)\nn=[] # negative samples (presumably contain entities that are possibly wrongly classified as entity)\nfor d in data:\n    if any(np.array(d[\"labels\"]) != \"O\"): p.append(d)\n    else: n.append(d)\nprint(\"original datapoints: \", len(data))\n\nexternal = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/pii_dataset_fixed.json\"))\nprint(\"external datapoints: \", len(external))\n\nmoredata = json.load(open(\"/kaggle/input/fix-punctuation-tokenization-external-dataset/moredata_dataset_fixed.json\"))\nprint(\"moredata datapoints: \", len(moredata))\n\ndata = moredata+external+p+n[:len(n)//3]\nprint(\"combined: \", len(data))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T22:12:09.050277Z","iopub.execute_input":"2024-01-27T22:12:09.051043Z","iopub.status.idle":"2024-01-27T22:12:15.609618Z","shell.execute_reply.started":"2024-01-27T22:12:09.051001Z","shell.execute_reply":"2024-01-27T22:12:15.608546Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"original datapoints:  6807\nexternal datapoints:  4434\nmoredata datapoints:  2000\ncombined:  9333\n","output_type":"stream"}]},{"cell_type":"code","source":"data[0].keys()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T22:17:58.730151Z","iopub.execute_input":"2024-01-27T22:17:58.730561Z","iopub.status.idle":"2024-01-27T22:17:58.737262Z","shell.execute_reply.started":"2024-01-27T22:17:58.730523Z","shell.execute_reply":"2024-01-27T22:17:58.736093Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"dict_keys(['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'])"},"metadata":{}}]},{"cell_type":"code","source":"data[0]['labels']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_labels = sorted(list(set(chain(*[x[\"labels\"] for x in data]))))\nlabel2id = {l: i for i,l in enumerate(all_labels)}\nid2label = {v:k for k,v in label2id.items()}\n\ntarget = [\n    'B-EMAIL', 'B-ID_NUM', 'B-NAME_STUDENT', 'B-PHONE_NUM', \n    'B-STREET_ADDRESS', 'B-URL_PERSONAL', 'B-USERNAME', 'I-ID_NUM', \n    'I-NAME_STUDENT', 'I-PHONE_NUM', 'I-STREET_ADDRESS', 'I-URL_PERSONAL'\n]\n\nprint(id2label)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:06:59.056697Z","iopub.execute_input":"2024-01-27T20:06:59.057048Z","iopub.status.idle":"2024-01-27T20:06:59.138528Z","shell.execute_reply.started":"2024-01-27T20:06:59.057019Z","shell.execute_reply":"2024-01-27T20:06:59.137650Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{0: 'B-EMAIL', 1: 'B-ID_NUM', 2: 'B-NAME_STUDENT', 3: 'B-PHONE_NUM', 4: 'B-STREET_ADDRESS', 5: 'B-URL_PERSONAL', 6: 'B-USERNAME', 7: 'I-ID_NUM', 8: 'I-NAME_STUDENT', 9: 'I-PHONE_NUM', 10: 'I-STREET_ADDRESS', 11: 'I-URL_PERSONAL', 12: 'O'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## ♟️ Data Tokenization\n- This tokenizer is actually special, comparing to usual NLP challenges","metadata":{}},{"cell_type":"code","source":"def tokenize(example, tokenizer, label2id, max_length):\n\n    # rebuild text from tokens\n    text = []\n    labels = []\n\n    for t, l, ws in zip(\n        example[\"tokens\"], example[\"provided_labels\"], example[\"trailing_whitespace\"]\n    ):\n        text.append(t)\n        labels.extend([l] * len(t))\n\n        if ws:\n            text.append(\" \")\n            labels.append(\"O\")\n\n    # actual tokenization\n    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, max_length=max_length)\n\n    labels = np.array(labels)\n\n    text = \"\".join(text)\n    token_labels = []\n\n    for start_idx, end_idx in tokenized.offset_mapping:\n        # CLS token\n        if start_idx == 0 and end_idx == 0:\n            token_labels.append(label2id[\"O\"])\n            continue\n\n        # case when token starts with whitespace\n        if text[start_idx].isspace():\n            start_idx += 1\n\n        token_labels.append(label2id[labels[start_idx]])\n\n    length = len(tokenized.input_ids)\n\n    return {**tokenized, \"labels\": token_labels, \"length\": length}","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:07:05.152903Z","iopub.execute_input":"2024-01-27T20:07:05.153802Z","iopub.status.idle":"2024-01-27T20:07:05.189318Z","shell.execute_reply.started":"2024-01-27T20:07:05.153765Z","shell.execute_reply":"2024-01-27T20:07:05.188221Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(TRAINING_MODEL_PATH)\n\nds = Dataset.from_dict({\n    \"full_text\": [x[\"full_text\"] for x in data],\n    \"document\": [str(x[\"document\"]) for x in data],\n    \"tokens\": [x[\"tokens\"] for x in data],\n    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n    \"provided_labels\": [x[\"labels\"] for x in data],\n})\nds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer, \"label2id\": label2id, \"max_length\": TRAINING_MAX_LENGTH}, num_proc=3)\n# ds = ds.class_encode_column(\"group\")","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:07:07.618321Z","iopub.execute_input":"2024-01-27T20:07:07.618699Z","iopub.status.idle":"2024-01-27T20:09:01.700188Z","shell.execute_reply.started":"2024-01-27T20:07:07.618667Z","shell.execute_reply":"2024-01-27T20:09:01.698968Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9bebef7356843bba5deb9692d0aaa52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e67797763f44c687acb5468a86cd3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07f7ca3f558a42bfb3b849be773d6b76"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/3111 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef1848605c3743a3854ae6f7263c7017"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/3111 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2513df528a4a44c3b2fc839f7bd67f88"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/3111 [00:00<?, ?ex/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f07b86d1002547b88f3039e7784db16b"}},"metadata":{}},{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"x = ds[0]\n\nfor t,l in zip(x[\"tokens\"], x[\"provided_labels\"]):\n    if l != \"O\":\n        print((t,l))\n\nprint(\"*\"*100)\n\nfor t, l in zip(tokenizer.convert_ids_to_tokens(x[\"input_ids\"]), x[\"labels\"]):\n    if id2label[l] != \"O\":\n        print((t,id2label[l]))","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:09:01.702655Z","iopub.execute_input":"2024-01-27T20:09:01.703686Z","iopub.status.idle":"2024-01-27T20:09:01.717588Z","shell.execute_reply.started":"2024-01-27T20:09:01.703642Z","shell.execute_reply":"2024-01-27T20:09:01.716651Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"('Richard', 'B-NAME_STUDENT')\n('Chang', 'B-NAME_STUDENT')\n('gwilliams@yahoo.com', 'B-EMAIL')\n('Richard', 'B-NAME_STUDENT')\n('Richard', 'B-NAME_STUDENT')\n('Richard', 'B-NAME_STUDENT')\n('711', 'B-STREET_ADDRESS')\n('Golden', 'I-STREET_ADDRESS')\n('Overpass', 'I-STREET_ADDRESS')\n('West', 'I-STREET_ADDRESS')\n('Andreaville', 'I-STREET_ADDRESS')\n('OH', 'I-STREET_ADDRESS')\n('Richard', 'B-NAME_STUDENT')\n('Richard', 'B-NAME_STUDENT')\n('Richard', 'B-NAME_STUDENT')\n****************************************************************************************************\n('▁Richard', 'B-NAME_STUDENT')\n('▁Chang', 'B-NAME_STUDENT')\n('▁g', 'B-EMAIL')\n('william', 'B-EMAIL')\n('s', 'B-EMAIL')\n('@', 'B-EMAIL')\n('yahoo', 'B-EMAIL')\n('.', 'B-EMAIL')\n('com', 'B-EMAIL')\n('▁Richard', 'B-NAME_STUDENT')\n('▁Richard', 'B-NAME_STUDENT')\n('▁Richard', 'B-NAME_STUDENT')\n('▁711', 'B-STREET_ADDRESS')\n('▁Golden', 'I-STREET_ADDRESS')\n('▁Over', 'I-STREET_ADDRESS')\n('pass', 'I-STREET_ADDRESS')\n('▁West', 'I-STREET_ADDRESS')\n('▁Andrea', 'I-STREET_ADDRESS')\n('ville', 'I-STREET_ADDRESS')\n('▁OH', 'I-STREET_ADDRESS')\n('▁Richard', 'B-NAME_STUDENT')\n('▁Richard', 'B-NAME_STUDENT')\n('▁Richard', 'B-NAME_STUDENT')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## 🧮 Competition metrics\n- Note that we are not using the normal F1 score.\n- Although it is early in the competition, there are plenty of discsussions already explaining this:\n- e.g., here: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/470024","metadata":{}},{"cell_type":"code","source":"from seqeval.metrics import recall_score, precision_score\nfrom seqeval.metrics import classification_report\nfrom seqeval.metrics import f1_score\n\ndef compute_metrics(p, all_labels):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [all_labels[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [all_labels[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    \n    recall = recall_score(true_labels, true_predictions)\n    precision = precision_score(true_labels, true_predictions)\n    f1_score = (1 + 5*5) * recall * precision / (5*5*precision + recall)\n    \n    results = {\n        'recall': recall,\n        'precision': precision,\n        'f1': f1_score\n    }\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:10:52.929191Z","iopub.execute_input":"2024-01-27T20:10:52.929596Z","iopub.status.idle":"2024-01-27T20:10:52.945068Z","shell.execute_reply.started":"2024-01-27T20:10:52.929563Z","shell.execute_reply":"2024-01-27T20:10:52.944203Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForTokenClassification.from_pretrained(\n    TRAINING_MODEL_PATH,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id,\n    ignore_mismatched_sizes=True\n)\ncollator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:10:54.601950Z","iopub.execute_input":"2024-01-27T20:10:54.602675Z","iopub.status.idle":"2024-01-27T20:10:56.903903Z","shell.execute_reply.started":"2024-01-27T20:10:54.602640Z","shell.execute_reply":"2024-01-27T20:10:56.902985Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d5125249e5c4fb3bd70654b6dffa84f"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# I decided to uses no eval\n# final_ds = ds.train_test_split(test_size=0.2, seed=42) # cannot use stratify_by_column='group'\n# final_ds","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:10:59.454380Z","iopub.execute_input":"2024-01-27T20:10:59.455210Z","iopub.status.idle":"2024-01-27T20:10:59.459439Z","shell.execute_reply.started":"2024-01-27T20:10:59.455172Z","shell.execute_reply":"2024-01-27T20:10:59.458423Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## 🏋🏻‍♀️ Training\n- I actually do not use an eval set for submission to train on all data\n- Values are not really tuned and go by gut feeling, as this is my first iteration / baseline","metadata":{}},{"cell_type":"code","source":"# I actually chose to not use any validation set. This is only for the model I use for submission.\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR, \n    fp16=True,\n    learning_rate=2e-5,\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n    report_to=\"none\",\n    evaluation_strategy=\"no\",\n    do_eval=False,\n    save_total_limit=1,\n    logging_steps=20,\n    lr_scheduler_type='cosine',\n    metric_for_best_model=\"f1\",\n    greater_is_better=True,\n    warmup_ratio=0.1,\n    weight_decay=0.01\n)\n\ntrainer = Trainer(\n    model=model, \n    args=args, \n    train_dataset=ds,\n    data_collator=collator, \n    tokenizer=tokenizer,\n    compute_metrics=partial(compute_metrics, all_labels=all_labels),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:11:00.588631Z","iopub.execute_input":"2024-01-27T20:11:00.589587Z","iopub.status.idle":"2024-01-27T20:11:01.024443Z","shell.execute_reply.started":"2024-01-27T20:11:00.589540Z","shell.execute_reply":"2024-01-27T20:11:01.023277Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-27T20:11:01.927245Z","iopub.execute_input":"2024-01-27T20:11:01.927934Z","iopub.status.idle":"2024-01-27T21:29:03.097032Z","shell.execute_reply.started":"2024-01-27T20:11:01.927899Z","shell.execute_reply":"2024-01-27T21:29:03.095808Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1749/1749 1:17:56, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>2.429800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.348800</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.411700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.137600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.084200</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.050700</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.037800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.031300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.020700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.020100</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.013400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.016600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.013500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.013600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.015000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.008700</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.009100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.009200</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.011700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.014300</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.009500</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.008100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.006200</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.006400</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.005400</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.007000</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.004300</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.006600</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.004700</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.005000</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.006800</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.006000</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.007600</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.006300</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.005500</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.005600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.009900</td>\n    </tr>\n    <tr>\n      <td>1020</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1040</td>\n      <td>0.007400</td>\n    </tr>\n    <tr>\n      <td>1060</td>\n      <td>0.004100</td>\n    </tr>\n    <tr>\n      <td>1080</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1120</td>\n      <td>0.005200</td>\n    </tr>\n    <tr>\n      <td>1140</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1160</td>\n      <td>0.003800</td>\n    </tr>\n    <tr>\n      <td>1180</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1220</td>\n      <td>0.001100</td>\n    </tr>\n    <tr>\n      <td>1240</td>\n      <td>0.003700</td>\n    </tr>\n    <tr>\n      <td>1260</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1280</td>\n      <td>0.005100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1320</td>\n      <td>0.004600</td>\n    </tr>\n    <tr>\n      <td>1340</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>1360</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1380</td>\n      <td>0.002000</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.004000</td>\n    </tr>\n    <tr>\n      <td>1420</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1440</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1460</td>\n      <td>0.002100</td>\n    </tr>\n    <tr>\n      <td>1480</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.004500</td>\n    </tr>\n    <tr>\n      <td>1520</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>1540</td>\n      <td>0.001500</td>\n    </tr>\n    <tr>\n      <td>1560</td>\n      <td>0.003500</td>\n    </tr>\n    <tr>\n      <td>1580</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>0.003600</td>\n    </tr>\n    <tr>\n      <td>1620</td>\n      <td>0.005300</td>\n    </tr>\n    <tr>\n      <td>1640</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1660</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>1680</td>\n      <td>0.004800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>0.003200</td>\n    </tr>\n    <tr>\n      <td>1720</td>\n      <td>0.006500</td>\n    </tr>\n    <tr>\n      <td>1740</td>\n      <td>0.003600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"CPU times: user 1h 46min 27s, sys: 26min 48s, total: 2h 13min 16s\nWall time: 1h 18min 1s\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1749, training_loss=0.057461005176021894, metrics={'train_runtime': 4680.5974, 'train_samples_per_second': 5.982, 'train_steps_per_second': 0.374, 'total_flos': 1.2183608904271872e+16, 'train_loss': 0.057461005176021894, 'epoch': 3.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"## 💾 Save models\n- You can click on \"Save version\" (top right) and \"Save & Run All (Commit)\"\n- Then you can use this notebook as input for your inference notebook","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"deberta3base_1024\")\ntokenizer.save_pretrained(\"deberta3base_1024\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}